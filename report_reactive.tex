\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
% add other packages here

% put your group number and names in the author field
\title{\bf Exercise 2: A Reactive Agent for the Pickup and Delivery Problem}
\author{Group \textnumero: Student 1, Student 2}

% the report should not be longer than 3 pages

\begin{document}
\maketitle

\section{Problem Representation}

\subsection{Representation Description}
% describe how you design the state representation, the possible actions, the reward table and the probability transition table

At the end of every action, an agent must be in a city. In that sense, every city represents a state. For the France topology this means a total of 9 different states. And in every city, the agent can either accepts the pickup task (if there is one) or move to a neighboring city to find a better pickup task. Thus, the two action possible will be accept the pickup task (in that case he goes to the delivery city using the shortest path) and moving to a neighbor city. The reward table will be a 9*2 table. For the probability transition table, it will then consists of a 9*2*9 table. .

\subsection{Implementation Details}
% describe the implementation details of the representations above and the implementation details of the reinforcement learning algorithm you implemented

In the reward table, when the action was doing the task, the reward was calculated as the reward from the initial city to the the final city minus the cost of going to the final city, everything multiplied by the probability that there is a task from this city to the other city. We sum this value over all the final state possible. When the action was moving to a neighbor city, the reward was negative and equal to the average cost it take to go in a neighbor city. In the case of the probability transition table, for the action "doing the pickup task", the probability will represent the probability that the final state city is the delivery knowing that the pickup city is the initial state city. For the action "move to a neighbor city" the probability will be zero if the city is not a neighbor and ... otherwise. At the end every probability is normalized so that the sum over all probability of final state for one initial state and one action should be one.

\section{Results}
% in this section, you describe several results from the experiments with your reactive agent

\subsection{Experiment 1: Discount factor}
% the purpose of this experiment is to understand how the discount factor influences the result

\subsubsection{Setting}
% you describe how you perform the experiment (you also need to specify the configuration used for the experiment)

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results

\subsection{Experiment 2: Comparisons with dummy agents}
% you compare the results of your agent with two dummy agents: the random agent that was already given in the starter files and another dummy agent that you define and create. You should report the results from the simulations using the topologies given in the starter files and optionally, additional topologies that you create.

\subsubsection{Setting}
% you describe how you perform the experiment and you describe the dummy agent you created (you also need to specify the configuration used for the experiment)

\subsubsection{Observations}
% elaborate on the observed results

\vdots

\subsection{Experiment n}
% other experiments you would like to present

\subsubsection{Setting}

\subsubsection{Observations}

\end{document}